# -*- coding: utf-8 -*-
"""SMS_Spam_Detection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1V5TZgREYDu8WcnLjGvnkhBktPSDs1Iow
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

# Reading the data
df = pd.read_csv("/content/sample_data/spam.csv", encoding='latin-1')
df.head()

"""This line of code removes unneccesary coloumns named unnamed:2, unnamed:3, unnamed:4 and rename columns v1 and v2 to label and text

"""

df = df.drop(['Unnamed: 2','Unnamed: 3','Unnamed: 4'],axis=1)
df = df.rename(columns={'v1':'label','v2':'Text'})
df['label_enc'] = df['label'].map({'ham':0,'spam':1})
df.head()

""":This line of code calculates the frequency of each unique value in the 'label' column

"""

df['label'].value_counts().plot(kind='bar')

"""This code groups the DataFrame df by the 'label' column, calculates the size of each group, and then plots a horizontal bar chart using the 'Dark2' color palette from Matplotlib

"""

df.groupby('label').size().plot(kind='barh', color=sns.palettes.mpl_palette('Dark2'))
plt.gca().spines[['top', 'right',]].set_visible(False)

"""It generates and displays a density plot using these text lengths to show variations in text length across different labels.

"""

df['text_length'] = df['Text'].apply(len)

plt.figure(figsize=(10, 6))
sns.kdeplot(data=df, x='text_length', hue='label', fill=True)
plt.title('2-d categorical distributions')
plt.xlabel('Length of Text')
plt.ylabel('Density')
plt.show()

"""This displays barplot which shows frequency of each unique values in label column.

"""

sns.countplot(x=df['label'])
plt.show()

"""This code calculates the average number of words per text message in the 'Text' column of the DataFrame df and prints the rounded result."""

avg_words_len=round(sum([len(i.split()) for i in df['Text']])/len(df['Text']))
print(avg_words_len)

"""This code iterates through each text message in the 'Text' column of the DataFrame df, splits each message into words, and adds these words to a set to ensure all entries are unique."""

s = set()
for sent in df['Text']:
  for word in sent.split():
	  s.add(word)
total_words_length=len(s)
print(total_words_length)

"""This code converts the 'Text' and 'label_enc' columns of the DataFrame df into numpy arrays, creates a new DataFrame new_df from these arrays, and then splits this new DataFrame into training and testing sets with 20% of the data reserved for testing."""

# Splitting data for Training and testing

from sklearn.model_selection import train_test_split

X, y = np.asanyarray(df['Text']), np.asanyarray(df['label_enc'])
new_df = pd.DataFrame({'Text': X, 'label': y})
X_train, X_test, y_train, y_test = train_test_split(
	new_df['Text'], new_df['label'], test_size=0.2, random_state=42)
X_train.shape, y_train.shape, X_test.shape, y_test.shape

"""This code sets up text classification using a Naive Bayes model by first converting the text data from the training and testing sets into TF-IDF vectors. It then trains a Multinomial Naive Bayes classifier on the vectorized training data to prepare for making predictions."""

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report,accuracy_score

tfidf_vec = TfidfVectorizer().fit(X_train)
X_train_vec,X_test_vec = tfidf_vec.transform(X_train),tfidf_vec.transform(X_test)

baseline_model = MultinomialNB()
baseline_model.fit(X_train_vec,y_train)

"""This line of code calculates the accuracy of the trained Multinomial Naive Bayes model by comparing data (X_test_vec) against the actual labels (y_test)"""

accuracy=accuracy_score(y_test,baseline_model.predict(X_test_vec))
print(accuracy)

print(classification_report(y_test,baseline_model.predict(X_test_vec)))

"""This code is performing text classification on a spam dataset using a Multinomial Naive Bayes model. It loads the dataset, preprocesses it, splits it into training and testing sets, vectorizes the text data, trains the model, makes predictions, calculates evaluation metrics (accuracy, precision, recall, and F1-score), creates a DataFrame to store the metrics, and finally visualizes the metrics using a bar plot.


"""

from tensorflow.keras.layers import TextVectorization

MAXTOKENS=total_words_length
OUTPUTLEN=avg_words_len

text_vec = TextVectorization(
	max_tokens=MAXTOKENS,
	standardize='lower_and_strip_punctuation',
	output_mode='int',
	output_sequence_length=OUTPUTLEN
)
text_vec.adapt(X_train)

"""This output shows the result of applying the TextVectorization layer to the sentence "This Is Ai final Project". The layer has converted the sentence into an array of integers, where each integer represents a token in the layer's vocabulary"""

sentence="This Is SMS Spam Detection Ai Final Project"
text_vec([sentence])

"""This code initializes an embedding layer for a neural network using TensorFlow's Keras library, which maps each word in a predefined vocabulary of a specific size to a 128-dimensional vector."""

embedding_layer = layers.Embedding(
	input_dim=MAXTOKENS,
	output_dim=128,
	embeddings_initializer='uniform',
	input_length=OUTPUTLEN
)

"""This code constructs a neural network model using TensorFlow's Keras API, designed for binary classification of text data. The model begins with an input layer that takes in strings, applies a text vectorization layer to convert these strings into integer sequences, and then uses an embedding layer to transform these sequences into dense vectors.


"""

input_layer = layers.Input(shape=(1,), dtype=tf.string)
vec_layer = text_vec(input_layer)
embedding_layer_model = embedding_layer(vec_layer)
x = layers.GlobalAveragePooling1D()(embedding_layer_model)
x = layers.Flatten()(x)
x = layers.Dense(32, activation='relu')(x)
output_layer = layers.Dense(1, activation='sigmoid')(x)
model_1 = keras.Model(input_layer, output_layer)

model_1.compile(optimizer='adam', loss=keras.losses.BinaryCrossentropy(
	label_smoothing=0.5), metrics=['accuracy'])

"""This line of code displays a summary of the neural network model model_1, providing details about each layer, including their types, shapes, and the number of parameters in the model, which helps in understanding the architecture and parameter count of the network.


"""

model_1.summary()

"""This code builds a TensorFlow Keras model for classifying text data. It starts with an input layer that accepts string data, processes it through a text vectorization layer to convert text to integer sequences, then uses an embedding layer to map these sequences to dense vectors. The model incorporates a global average pooling layer to simplify the output from the embedding layer, a flattening step, and a dense layer with ReLU activation for learning nonlinear interactions. It concludes with an output layer using sigmoid activation for binary classification. The model is compiled with the Adam optimizer and binary crossentropy loss with label smoothing to mitigate overfitting, while accuracy is tracked as a performance metric."""

input_layer = layers.Input(shape=(1,), dtype=tf.string)
vec_layer = text_vec(input_layer)
embedding_layer_model = embedding_layer(vec_layer)
x = layers.GlobalAveragePooling1D()(embedding_layer_model)
x = layers.Flatten()(x)
x = layers.Dense(32, activation='relu')(x)
output_layer = layers.Dense(1, activation='sigmoid')(x)
model_1 = keras.Model(input_layer, output_layer)

model_1.compile(optimizer='adam', loss=keras.losses.BinaryCrossentropy(
	label_smoothing=0.5), metrics=['accuracy'])

"""This Python script defines three functions for managing a machine learning model using the TensorFlow Keras framework. The compile_model function sets up the model with the Adam optimizer, binary crossentropy loss, and tracks accuracy as a metric. The fit_model function trains the model on provided training data for a specified number of epochs, validating performance on test data. The evaluate_model function assesses the model on given data, calculating and returning metrics like accuracy, precision, recall, and F1-score, summarizing the model's performance in a dictionary format.


"""

from sklearn.metrics import precision_score, recall_score, f1_score

def compile_model(model):
	'''
	simply compile the model with adam optimzer
	'''
	model.compile(optimizer=keras.optimizers.Adam(),
				loss=keras.losses.BinaryCrossentropy(),
				metrics=['accuracy'])

def fit_model(model, epochs, X_train, y_train, X_test, y_test):
	'''
	fit the model with given epochs, train
	and test data
	'''
	history = model.fit(X_train,
	                    y_train,
						          epochs=epochs,
						          validation_data=(X_test, y_test),
						          validation_steps=int(0.2*len(X_test)))
	return history

def evaluate_model(model, X, y):
	'''
	evaluate the model and returns accuracy,
	precision, recall and f1-score
	'''
	y_preds = np.round(model.predict(X))
	accuracy = accuracy_score(y, y_preds)
	precision = precision_score(y, y_preds)
	recall = recall_score(y, y_preds)
	f1 = f1_score(y, y_preds)

	model_results_dict = {'accuracy': accuracy,
						'precision': precision,
						'recall': recall,
						'f1-score': f1}

	return model_results_dict

"""This code constructs a more complex neural network model using TensorFlow's Keras API for text classification. It begins with an input layer for string data, followed by a text vectorization layer to convert text to integer sequences and an embedding layer to map these sequences to dense vectors. Then, it includes two bidirectional LSTM layers with 64 units each, which process the input sequence in both forward and backward directions to capture contextual information. Afterward, it flattens the output, applies dropout regularization, and passes it through a dense layer with ReLU activation. Finally, it ends with an output layer using a sigmoid activation function for binary classification. The model is compiled with the Adam optimizer and binary crossentropy loss and trained for 5 epochs on the provided data.


"""

# Splitting data for Training and testing

from sklearn.model_selection import train_test_split

X, y = np.asanyarray(df['Text']), np.asanyarray(df['label_enc'])
new_df = pd.DataFrame({'Text': X, 'label': y})
X_train, X_test, y_train, y_test = train_test_split(
    new_df['Text'], new_df['label'], test_size=0.2, random_state=42)
X_train.shape, y_train.shape, X_test.shape, y_test.shape

# Convert X_train and X_test to numpy arrays
X_train = X_train.to_numpy()
X_test = X_test.to_numpy()

input_layer = layers.Input(shape=(1,), dtype=tf.string)
vec_layer = text_vec(input_layer)
embedding_layer_model = embedding_layer(vec_layer)
bi_lstm = layers.Bidirectional(layers.LSTM(
    64, activation='tanh', return_sequences=True))(embedding_layer_model)
lstm = layers.Bidirectional(layers.LSTM(64))(bi_lstm)
flatten = layers.Flatten()(lstm)
dropout = layers.Dropout(.1)(flatten)
x = layers.Dense(32, activation='relu')(dropout)
output_layer = layers.Dense(1, activation='sigmoid')(x)
model_2 = keras.Model(input_layer, output_layer)

compile_model(model_2) # compile the model

# Now, call the fit_model function with the updated X_train and X_test
history_2 = fit_model(model_2, epochs=5, X_train=X_train, y_train=y_train, X_test=X_test,  y_test=y_test) # fit the model

"""This code utilizes TensorFlow and TensorFlow Hub to create a Sequential model for text classification, incorporating the Universal Sentence Encoder (USE) to embed text inputs into numerical vectors, followed by dropout regularization and two dense layers for classification, ultimately training the model over five epochs.


"""

import tensorflow_hub as hub
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn.model_selection import train_test_split
import numpy as np
import pandas as pd # Import pandas

# Assuming df is your DataFrame containing the data
X, y = np.asanyarray(df['Text']), np.asanyarray(df['label_enc'])
new_df = pd.DataFrame({'Text': X, 'label': y})
X_train, X_test, y_train, y_test = train_test_split(
    new_df['Text'], new_df['label'], test_size=0.2, random_state=42)
X_train.shape, y_train.shape, X_test.shape, y_test.shape

# Convert X_train and X_test to numpy arrays
X_train = X_train.to_numpy()
X_test = X_test.to_numpy()

model_3 = keras.Sequential()

use_layer = hub.KerasLayer(
    "https://tfhub.dev/google/universal-sentence-encoder/4",
    trainable=False,
    input_shape=[],
    dtype=tf.string,
    name="USE",
)

# Wrap the hub.KerasLayer with a lambda layer to make it compatible
model_3.add(layers.Lambda(lambda x: use_layer(x)))
model_3.add(layers.Dropout(0.2))
model_3.add(layers.Dense(64, activation=keras.activations.relu))
model_3.add(layers.Dense(1, activation=keras.activations.sigmoid))

compile_model(model_3)

history_3 = fit_model(
    model_3, epochs=5, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test
) # Pass X_train, y_train, X_test, and y_test to fit_model

"""This code evaluates the performance of multiple models (Multinomial Naive Bayes, Custom Vector Embedding, Bidirectional LSTM, and Transfer Learning with Universal Sentence Encoder) on a test dataset, aggregating the results into a DataFrame for comparison


"""

baseline_model_results = evaluate_model(baseline_model, X_test_vec, y_test)
model_1_results = evaluate_model(model_1, X_test, y_test)
model_2_results = evaluate_model(model_2, X_test, y_test)
model_3_results = evaluate_model(model_3, X_test, y_test)

total_results = pd.DataFrame({'MultinomialNB Model':baseline_model_results,
							'Custom-Vec-Embedding Model':model_1_results,
							'Bidirectional-LSTM Model':model_2_results,
							'USE-Transfer learning Model':model_3_results}).transpose()

total_results